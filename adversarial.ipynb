{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/paint/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import clip\n",
    "from transformers import CLIPImageProcessor, CLIPModel\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = [{'image' : str(s), 'label' : 1} for s in Path(\"train/fake\").glob(\"*\")]\n",
    "real = [{'image' : str(s), 'label' : 0} for s in Path(\"train/real\").glob(\"*\")]\n",
    "data = fake + real\n",
    "pd.DataFrame(data).to_json(\"train.json\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealFakeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, json_path, processor):\n",
    "        # Initialize image paths and corresponding texts\n",
    "        self.jsons = pd.read_json(json_path, lines=True)\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.jsons)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Preprocess image using CLIP's preprocessing function\n",
    "        image = self.processor(Image.open(self.jsons.iloc[idx]['image']))\n",
    "        label = torch.tensor(self.jsons.iloc[idx]['label'])\n",
    "        return image, label\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, ckpt=\"openai/clip-vit-base-patch32\"):\n",
    "        super().__init__()\n",
    "        self.model = CLIPModel.from_pretrained(ckpt).to(torch.float16)\n",
    "        self.linear = nn.Linear(512, 2, dtype=torch.float16)\n",
    "    def forward(self, x):\n",
    "        image_features = self.model.get_image_features(**x)\n",
    "        x = self.linear(image_features)\n",
    "        return x\n",
    "    def to(self, device):\n",
    "        self.model.to(device)\n",
    "        self.linear.to(device)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier().to(\"cuda\")\n",
    "processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "json_path = \"train.json\" \n",
    "train_dataset = RealFakeDataset(json_path, processor)\n",
    "def collate_fn(batch):\n",
    "    images = [torch.tensor(x[0]['pixel_values'][0]) for x in batch]\n",
    "    labels = [x[1] for x in batch]\n",
    "    images = {'pixel_values': torch.stack(images)}\n",
    "    labels = torch.stack(labels)\n",
    "    return images, labels\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/30, Loss: 0.3193: 100%|██████████| 22/22 [00:01<00:00, 11.04it/s]\n",
      "Epoch 1/30, Loss: 0.8389: 100%|██████████| 22/22 [00:02<00:00, 10.24it/s]\n",
      "Epoch 2/30, Loss: 0.7407: 100%|██████████| 22/22 [00:02<00:00, 10.85it/s]\n",
      "Epoch 3/30, Loss: 0.5903: 100%|██████████| 22/22 [00:02<00:00, 10.67it/s]\n",
      "Epoch 4/30, Loss: 0.3643: 100%|██████████| 22/22 [00:02<00:00, 10.76it/s]\n",
      "Epoch 5/30, Loss: 0.6782: 100%|██████████| 22/22 [00:02<00:00, 10.82it/s]\n",
      "Epoch 6/30, Loss: 0.9692: 100%|██████████| 22/22 [00:02<00:00, 10.62it/s]\n",
      "Epoch 7/30, Loss: 0.6157: 100%|██████████| 22/22 [00:02<00:00, 10.73it/s]\n",
      "Epoch 8/30, Loss: 0.3064:  82%|████████▏ | 18/22 [00:01<00:00, 11.35it/s]"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    pbar = tqdm(train_dataloader, total=len(train_dataloader))\n",
    "    for batch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        images, labels = batch \n",
    "        images['pixel_values'] = images['pixel_values'].to(\"cuda\")\n",
    "        labels = labels.to(\"cuda\")\n",
    "        # Forward pass\n",
    "        linear_output = model(images)\n",
    "\n",
    "        # Compute loss\n",
    "        cls_loss = loss(linear_output, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        cls_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pbar.set_description(f\"Epoch {epoch}/{num_epochs}, Loss: {cls_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
